{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"},{"sourceId":6924580,"sourceType":"datasetVersion","datasetId":3976011}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U xgboost -f /kaggle/input/xgboost-python-package/ --no-index","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-18T18:11:33.893497Z","iopub.execute_input":"2024-01-18T18:11:33.893987Z","iopub.status.idle":"2024-01-18T18:11:42.195803Z","shell.execute_reply.started":"2024-01-18T18:11:33.893951Z","shell.execute_reply":"2024-01-18T18:11:42.194540Z"},"editable":false,"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/xgboost-python-package/\nRequirement already satisfied: xgboost in /opt/conda/lib/python3.10/site-packages (2.0.2)\n\u001b[33mWARNING: Location '/kaggle/input/xgboost-python-package/' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.24.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.11.4)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom colorama import Fore, Style, init;\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport torch\n\n#Geolocation\nfrom geopy.geocoders import Nominatim\n\npd.set_option('display.max_columns', 100)\nDEBUG=False","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:42.198200Z","iopub.execute_input":"2024-01-18T18:11:42.199537Z","iopub.status.idle":"2024-01-18T18:11:49.952731Z","shell.execute_reply.started":"2024-01-18T18:11:42.199494Z","shell.execute_reply":"2024-01-18T18:11:49.949424Z"},"editable":false,"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#Geolocation\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeocoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Nominatim\n\u001b[1;32m     17\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     18\u001b[0m DEBUG\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/geopy/__init__.py:12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mgeopy is a Python client for several popular geocoding web services.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mand PyPy3. geopy 1.x line also supported CPython 2.7, 3.4 and PyPy2.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeocoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Location  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Point  \u001b[38;5;66;03m# noqa\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/geopy/geocoders/__init__.py:236\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeocoders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeolake\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Geolake\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeocoders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeonames\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeoNames\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeocoders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgoogle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleV3\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeocoders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhere\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Here, HereV7\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeocoders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mignfrance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IGNFrance\n","File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# using GPU or CPU\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.954273Z","iopub.status.idle":"2024-01-18T18:11:49.954812Z","shell.execute_reply.started":"2024-01-18T18:11:49.954580Z","shell.execute_reply":"2024-01-18T18:11:49.954603Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just some coloring and printing functions\n# first one prints the size and first row of the dataset\n# second one is just coloring\ndef display_df(df, name):\n    '''Display df shape and first row '''\n    PrintColor(text = f'{name} data has {df.shape[0]} rows and {df.shape[1]} columns. \\n ===> First row:')\n    display(df.head(1))\n\ndef PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n    '''Prints color outputs using colorama of a text string'''\n    print(style + color + text + Style.RESET_ALL); ","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.956221Z","iopub.status.idle":"2024-01-18T18:11:49.957042Z","shell.execute_reply.started":"2024-01-18T18:11:49.956824Z","shell.execute_reply":"2024-01-18T18:11:49.956847Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just reading all datasets\ntrain = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\")\nclient = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/client.csv\")\nhistorical_weather = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv\")\nforecast_weather = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv\")\nelectricity = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv\")\ngas = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv\")\nlocation = pd.read_csv(\"/kaggle/input/fabiendaniels-mapping-locations-and-county-codes/county_lon_lats.csv\").drop(columns=[\"Unnamed: 0\"])","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.958343Z","iopub.status.idle":"2024-01-18T18:11:49.958758Z","shell.execute_reply.started":"2024-01-18T18:11:49.958564Z","shell.execute_reply":"2024-01-18T18:11:49.958582Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using the coloring and printing functions to show datasets\ndisplay_df(train, \"train\")\ndisplay_df(client, \"client\")\ndisplay_df(historical_weather, \"historical_weather\")\ndisplay_df(forecast_weather, \"forecast_weather\")\ndisplay_df(electricity, \"electricity\")\ndisplay_df(gas, \"gas\")\ndisplay_df(location, \"location\")","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.960173Z","iopub.status.idle":"2024-01-18T18:11:49.960579Z","shell.execute_reply.started":"2024-01-18T18:11:49.960373Z","shell.execute_reply":"2024-01-18T18:11:49.960392Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# open the county id to name file (json format)\nwith open(\"/kaggle/input/predict-energy-behavior-of-prosumers/county_id_to_name_map.json\") as f:\n    county_codes = json.load(f)\npd.DataFrame(county_codes, index=[0])","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.961934Z","iopub.status.idle":"2024-01-18T18:11:49.962374Z","shell.execute_reply.started":"2024-01-18T18:11:49.962167Z","shell.execute_reply":"2024-01-18T18:11:49.962186Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureProcessorClass():\n    def __init__(self):         \n        # Take columns from different datasets \n        # and combine into one later: weather, client, electricity, gas\n        self.weather_join = ['datetime', 'county', 'data_block_id']\n        self.gas_join = ['data_block_id']\n        self.electricity_join = ['datetime', 'data_block_id']\n        self.client_join = ['county', 'is_business', 'product_type', 'data_block_id']\n        \n        # Columns of latitude & longitude\n        self.lat_lon_columns = ['latitude', 'longitude']\n        \n        # Aggregate stats \n        self.agg_stats = ['mean'] #, 'min', 'max', 'std', 'median']\n        \n        # Categorical columns (specify for XGBoost)\n        self.category_columns = ['county', 'is_business', 'product_type', 'is_consumption', 'data_block_id']\n\n    def create_new_column_names(self, df, suffix, columns_no_change):\n        # Change column names by given suffix, check column name\n        # if it is not in columns_no_change, change name into col + suffix\n        # and return the data\n        df.columns = [col + suffix \n                      if col not in columns_no_change\n                      else col\n                      for col in df.columns\n                      ]\n        return df \n\n    def flatten_multi_index_columns(self, df):\n        # list A: (if it is not empty) column names\n        # list B is the final list: for each column, \"_\".join(list A)\n        df.columns = ['_'.join([col for col in multi_col if len(col)>0]) \n                      for multi_col in df.columns]\n        return df\n    \n    def create_data_features(self, data):\n        # format all the datetime\n        \n        # To datetime\n        data['datetime'] = pd.to_datetime(data['datetime'])\n        \n        # Time period features\n        data['date'] = data['datetime'].dt.normalize()\n        data['year'] = data['datetime'].dt.year\n        data['quarter'] = data['datetime'].dt.quarter\n        data['month'] = data['datetime'].dt.month\n        data['week'] = data['datetime'].dt.isocalendar().week\n        data['hour'] = data['datetime'].dt.hour\n        \n        # Day features\n        data['day_of_year'] = data['datetime'].dt.day_of_year\n        data['day_of_month']  = data['datetime'].dt.day\n        data['day_of_week'] = data['datetime'].dt.day_of_week\n        return data\n\n    def create_client_features(self, client):\n        # Apply first two def to change some of the names in client list\n        client = self.create_new_column_names(client, \n                                           suffix='_client',\n                                           columns_no_change = self.client_join\n                                          )       \n        return client\n    \n    def create_historical_weather_features(self, historical_weather):\n        # Formuate the datetime\n        historical_weather['datetime'] = pd.to_datetime(historical_weather['datetime'])\n        \n        # Add column county into historical weather\n        historical_weather[self.lat_lon_columns] = historical_weather[self.lat_lon_columns].astype(float).round(1)\n        historical_weather = historical_weather.merge(location, how = 'left', on = self.lat_lon_columns)\n\n        # Apply first two def to change some of the names in historical weather list\n        # the column no change is historical weather + location\n        historical_weather = self.create_new_column_names(historical_weather,\n                                                          suffix='_h',\n                                                          columns_no_change = self.lat_lon_columns + self.weather_join\n                                                          ) \n        \n        # for column in hist weather, if it is not in the combined list, append into agg_columns\n        # make a dict that each col in agg_columns has the agg statistical info: see agg_stat in the first def\n        # grouping according to the self.weather_join\n        agg_columns = [col for col in historical_weather.columns if col not in self.lat_lon_columns + self.weather_join]\n        agg_dict = {agg_col: self.agg_stats for agg_col in agg_columns}\n        historical_weather = historical_weather.groupby(self.weather_join).agg(agg_dict).reset_index() \n        \n        # Flatten the multi column aggregates\n        historical_weather = self.flatten_multi_index_columns(historical_weather) \n        \n        # Test set has 1 day offset for hour<11 and 2 day offset for hour>11\n        historical_weather['hour_h'] = historical_weather['datetime'].dt.hour\n        historical_weather['datetime'] = (historical_weather\n                                               .apply(lambda x: \n                                                      x['datetime'] + pd.DateOffset(1) \n                                                      if x['hour_h']< 11 \n                                                      else x['datetime'] + pd.DateOffset(2),\n                                                      axis=1)\n                                              )\n        \n        return historical_weather\n    \n    def create_forecast_weather_features(self, forecast_weather):\n        # drop the origin_date and rename the columns\n        forecast_weather = (forecast_weather\n                            .rename(columns = {'forecast_datetime': 'datetime'})\n                            .drop(columns = 'origin_datetime')\n                           )\n        \n        # format the datetime\n        forecast_weather['datetime'] = (pd.to_datetime(forecast_weather['datetime'])\n                                        .dt\n                                        .tz_localize(None)\n                                       )\n\n        # add county into forecast_weather dataset\n        # add the location info into the dataset\n        forecast_weather[self.lat_lon_columns] = forecast_weather[self.lat_lon_columns].astype(float).round(1)\n        forecast_weather = forecast_weather.merge(location, how = 'left', on = self.lat_lon_columns)\n        \n        # Apply first two def to change some of the names in forecast weather and lat_lon\n        forecast_weather = self.create_new_column_names(forecast_weather,\n                                                        suffix='_f',\n                                                        columns_no_change = self.lat_lon_columns + self.weather_join\n                                                        ) \n        \n        # for column in fore. weather, if it is not in the combined list, append into agg_columns\n        # make a dict that each col in agg_columns has the agg statistical info: see agg_stat in the first def\n        # grouping according to the self.weather_join\n        agg_columns = [col for col in forecast_weather.columns if col not in self.lat_lon_columns + self.weather_join]\n        agg_dict = {agg_col: self.agg_stats for agg_col in agg_columns}\n        forecast_weather = forecast_weather.groupby(self.weather_join).agg(agg_dict).reset_index() \n        \n        # Flatten the multi column aggregates\n        forecast_weather = self.flatten_multi_index_columns(forecast_weather)     \n        return forecast_weather\n\n    def create_electricity_features(self, electricity):\n        # Format datetime in electricity\n        electricity['forecast_date'] = pd.to_datetime(electricity['forecast_date'])\n        \n        # Test set has 1 day offset\n        electricity['datetime'] = electricity['forecast_date'] + pd.DateOffset(1)\n        \n        # Apply first two def to change names in electricity\n        electricity = self.create_new_column_names(electricity, \n                                                   suffix='_electricity',\n                                                   columns_no_change = self.electricity_join\n                                                  )             \n        return electricity\n\n    def create_gas_features(self, gas):\n        # Mean gas price\n        gas['mean_price_per_mwh'] = (gas['lowest_price_per_mwh'] + gas['highest_price_per_mwh'])/2\n        \n        # Apply first two def to change names in gas\n        gas = self.create_new_column_names(gas, \n                                           suffix='_gas',\n                                           columns_no_change = self.gas_join\n                                          )       \n        return gas\n    \n    def __call__(self, data, client, historical_weather, forecast_weather, electricity, gas):\n        # Create and prepare features for all dataset\n        data = self.create_data_features(data)\n        client = self.create_client_features(client)\n        historical_weather = self.create_historical_weather_features(historical_weather)\n        forecast_weather = self.create_forecast_weather_features(forecast_weather)\n        electricity = self.create_electricity_features(electricity)\n        gas = self.create_gas_features(gas)\n        \n        # Now add all dataset into one:\n        df = data.merge(client, how='left', on = self.client_join)\n        df = df.merge(historical_weather, how='left', on = self.weather_join)\n        df = df.merge(forecast_weather, how='left', on = self.weather_join)\n        df = df.merge(electricity, how='left', on = self.electricity_join)\n        df = df.merge(gas, how='left', on = self.gas_join)\n        \n        # Change columns to categorical for XGBoost\n        df[self.category_columns] = df[self.category_columns].astype('category')\n        return df","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.966102Z","iopub.status.idle":"2024-01-18T18:11:49.966677Z","shell.execute_reply.started":"2024-01-18T18:11:49.966487Z","shell.execute_reply":"2024-01-18T18:11:49.966506Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_revealed_targets_train(data, N_day_lags):\n    # copy necessary info\n    original_datetime = data['datetime']\n    revealed_targets = data[['datetime', 'prediction_unit_id', 'is_consumption', 'target']].copy()\n    \n    # first calculate the \n    for day_lag in range(2, N_day_lags+1):\n        revealed_targets['datetime'] = original_datetime + pd.DateOffset(day_lag)\n        # now keep the original and using on =\"List\" as pivot\n        # only add target_days_ago (original is left)\n        data = data.merge(revealed_targets, \n                          how='left', \n                          on = ['datetime', 'prediction_unit_id', 'is_consumption'],\n                          suffixes = ('', f'_{day_lag}_days_ago')\n                         )\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.967737Z","iopub.status.idle":"2024-01-18T18:11:49.968328Z","shell.execute_reply.started":"2024-01-18T18:11:49.968120Z","shell.execute_reply":"2024-01-18T18:11:49.968147Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# control the number of days to train (at least 2 days)\nN_day_lags = 15\n\nFeatureProcessor = FeatureProcessorClass()\n\ndata = FeatureProcessor(data = train.copy(),\n                      client = client.copy(),\n                      historical_weather = historical_weather.copy(),\n                      forecast_weather = forecast_weather.copy(),\n                      electricity = electricity.copy(),\n                      gas = gas.copy(),\n                     )\n\ndf = create_revealed_targets_train(data.copy(), \n                                  N_day_lags = N_day_lags)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.969248Z","iopub.status.idle":"2024-01-18T18:11:49.970058Z","shell.execute_reply.started":"2024-01-18T18:11:49.969848Z","shell.execute_reply":"2024-01-18T18:11:49.969870Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"target\"\n# cleaning, ditch null in target & reset index\ndf = df[df[target].notnull()].reset_index(drop=True)\n\ntrain_block_id = list(range(0, 500))\n\n# first 500 data_block_ids are used for training, if the \n# data_block_id IS IN the list, then it is train set, or else test set\ntr = df[df[\"data_block_id\"].isin(train_block_id)]\n\n#others are for validation\nval = df[~df[\"data_block_id\"].isin(train_block_id)]","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.971322Z","iopub.status.idle":"2024-01-18T18:11:49.972462Z","shell.execute_reply.started":"2024-01-18T18:11:49.972158Z","shell.execute_reply":"2024-01-18T18:11:49.972189Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_features = ['date', \n                'latitude', \n                'longitude', \n                'data_block_id', \n                'row_id',\n                'hours_ahead',\n                'hour_h',\n               ]\n\nremove_columns = []\nfeatures = []\n\n# prepare the feature we want to use, delete the unnecessary ones:\n# gather the columns in df that is unnecessary for analysis\nfor col in df.columns:\n    for no_feature in no_features:\n        if no_feature in col:\n            remove_columns.append(col)\n\nremove_columns.append(target)\n\n# extract the feature that is necessary\nfor col in df.columns:\n    if col not in remove_columns:\n        features.append(col)\n        \nPrintColor(f'There are {len(features)} features for training: {features}')","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.973779Z","iopub.status.idle":"2024-01-18T18:11:49.974350Z","shell.execute_reply.started":"2024-01-18T18:11:49.974043Z","shell.execute_reply":"2024-01-18T18:11:49.974091Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr['week']=tr['week'].astype(np.int32)\nval['week']=val['week'].astype(np.int32)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.975652Z","iopub.status.idle":"2024-01-18T18:11:49.976211Z","shell.execute_reply.started":"2024-01-18T18:11:49.975916Z","shell.execute_reply":"2024-01-18T18:11:49.975942Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. XGBoost approach**","metadata":{"editable":false}},{"cell_type":"code","source":"%%time\n# set up xgb\nxgb_reg = xgb.XGBRegressor(device = device, enable_categorical=True, objective = \"reg:absoluteerror\", n_estimators = 2 if DEBUG else 1500)#, early_stopping_rounds=100)\n# fitting\nxgb_reg.fit(X = tr[features], y = tr[target], eval_set = [(tr[features], tr[target]), (val[features], val[target])], verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.978153Z","iopub.status.idle":"2024-01-18T18:11:49.978539Z","shell.execute_reply.started":"2024-01-18T18:11:49.978352Z","shell.execute_reply":"2024-01-18T18:11:49.978371Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the best iteration that has smallest MAE\nprint('Early stopping on best iteration #', xgb_reg.best_iteration, 'with MAE error on validation set of', xgb_reg.best_score)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.979705Z","iopub.status.idle":"2024-01-18T18:11:49.980115Z","shell.execute_reply.started":"2024-01-18T18:11:49.979891Z","shell.execute_reply":"2024-01-18T18:11:49.979908Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting both training and validation MAE\nresults = xgb_reg.evals_result_\ntrain_mae, val_mae = results['validation_0'][\"mae\"], results['validation_1'][\"mae\"]\nitern=list(range(len(train_mae)))\n\nplt.plot(itern, train_mae, label=\"Training MAE\")\nplt.plot(itern, val_mae, label=\"Validation MAE\")\nplt.title(\"XGBRegressor: MAE vs Iteration number\")\nplt.xlabel(\"Iteration number\")\nplt.ylabel(\"MAE\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.981824Z","iopub.status.idle":"2024-01-18T18:11:49.982645Z","shell.execute_reply.started":"2024-01-18T18:11:49.982346Z","shell.execute_reply":"2024-01-18T18:11:49.982375Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the top 20 important features\n# starting from the most important one\ndata_imp = pd.DataFrame({\"feature_name\": xgb_reg.feature_names_in_, \"importance\": xgb_reg.feature_importances_})\ndata_imp\n\nsorted_data_imp = data_imp.sort_values(by=\"importance\", ascending=False)[:20].plot.barh(y=\"importance\", x=\"feature_name\", label = \"importance\")\nsorted_data_imp.invert_yaxis()","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.984129Z","iopub.status.idle":"2024-01-18T18:11:49.984662Z","shell.execute_reply.started":"2024-01-18T18:11:49.984391Z","shell.execute_reply":"2024-01-18T18:11:49.984417Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. LightBGM","metadata":{"editable":false}},{"cell_type":"code","source":"%%time\n# set up lgb\ngbm_reg = lgb.LGBMRegressor(objecive= \"regression_l1\", n_estimators = 2 if DEBUG else 1500)#, early_stopping_rounds = 100)\n# fitting\ngbm_reg.fit(tr[features], tr[target], eval_set = [(tr[features], tr[target]), (val[features], val[target])], eval_metric=\"mae\")#, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.987525Z","iopub.status.idle":"2024-01-18T18:11:49.988365Z","shell.execute_reply.started":"2024-01-18T18:11:49.988078Z","shell.execute_reply":"2024-01-18T18:11:49.988111Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the best iteration that has smallest MAE\nprint('Early stopping on best iteration #', gbm_reg.best_iteration_, 'with MAE error on validation set of', gbm_reg.best_score_['valid_1']['l1'])","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.989828Z","iopub.status.idle":"2024-01-18T18:11:49.990272Z","shell.execute_reply.started":"2024-01-18T18:11:49.990045Z","shell.execute_reply":"2024-01-18T18:11:49.990083Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting both training and validation MAE\nresults2 = gbm_reg.evals_result_\ntrain2_mae, val2_mae = results2['valid_0'][\"l1\"], results2['valid_1'][\"l1\"]\nitern2=list(range(len(train2_mae)))\n\nplt.plot(itern2, train2_mae, label=\"Training MAE\")\nplt.plot(itern2, val2_mae, label=\"Validation MAE\")\nplt.title(\"LGBMRegressor: MAE vs Iteration number\")\nplt.xlabel(\"Iteration number\")\nplt.ylabel(\"MAE\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.991454Z","iopub.status.idle":"2024-01-18T18:11:49.991858Z","shell.execute_reply.started":"2024-01-18T18:11:49.991665Z","shell.execute_reply":"2024-01-18T18:11:49.991683Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the top 20 important features\n# starting from the most important one\n# normalized before showing\ngbm_reg_imp=gbm_reg.feature_importances_/np.sum(gbm_reg.feature_importances_)\ndata_imp2 = pd.DataFrame({\"feature_name\": gbm_reg.feature_name_, \"importance\": gbm_reg_imp})\ndata_imp2\n\nsorted_data_imp2 = data_imp2.sort_values(by=\"importance\", ascending=False)[:20].plot.barh(y=\"importance\", x=\"feature_name\", label = \"importance\")\nsorted_data_imp2.invert_yaxis()","metadata":{"execution":{"iopub.status.busy":"2024-01-18T18:11:49.993616Z","iopub.status.idle":"2024-01-18T18:11:49.994052Z","shell.execute_reply.started":"2024-01-18T18:11:49.993835Z","shell.execute_reply":"2024-01-18T18:11:49.993854Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}